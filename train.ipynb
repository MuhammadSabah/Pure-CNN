{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ All libraries imported successfully!\n",
            "📦 Pure CNN implementation loaded from src/\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import sys\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from IPython.display import display, clear_output\n",
        "import time\n",
        "\n",
        "sys.path.append(os.path.join(os.getcwd(), 'src'))\n",
        "\n",
        "from cnn import CNN, Adam\n",
        "from data import DataLoader\n",
        "from utils import calculate_accuracy, plot_training_history\n",
        "\n",
        "print(\"✅ All libraries imported successfully!\")\n",
        "print(\"📦 Pure CNN implementation loaded from src/\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🎛️ Configuration set:\n",
            "   Epochs: 8\n",
            "   Batch size: 32\n",
            "   Learning rate: 0.003\n",
            "   Image size: 28x28\n",
            "   Max images per class: ALL\n",
            "🎯 TARGET: 80% accuracy with reasonable speed!\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# 🎛️ CONFIGURATION - Modify these settings as needed\n",
        "# ===================================================================\n",
        "\n",
        "# Training parameters - BALANCED FOR 80% ACCURACY + SPEED\n",
        "EPOCHS = 8                    # Enough epochs for good learning\n",
        "BATCH_SIZE = 32              # Good batch size\n",
        "LEARNING_RATE = 0.003         # Balanced learning rate\n",
        "IMAGE_SIZE = 28               # Small but sufficient for 80% accuracy\n",
        "\n",
        "# Data parameters  \n",
        "DATA_DIR = 'data'                      # Directory containing cat and dog folders\n",
        "MAX_IMAGES_PER_CLASS = None           # Use ALL available images!\n",
        "\n",
        "# Display settings\n",
        "PLOT_DURING_TRAINING = True           # Show live training plots\n",
        "SAVE_PLOTS = True                     # Save plots to files\n",
        "\n",
        "print(\"🎛️ Configuration set:\")\n",
        "print(f\"   Epochs: {EPOCHS}\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Learning rate: {LEARNING_RATE}\")\n",
        "print(f\"   Image size: {IMAGE_SIZE}x{IMAGE_SIZE}\")\n",
        "print(f\"   Max images per class: {'ALL' if MAX_IMAGES_PER_CLASS is None else MAX_IMAGES_PER_CLASS}\")\n",
        "print(\"🎯 TARGET: 80% accuracy with reasonable speed!\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Created directories: saved_models/, plots/\n",
            "✅ Data directories found!\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# 📁 Setup and Data Loading\n",
        "# ===================================================================\n",
        "\n",
        "\n",
        "os.makedirs('saved_models', exist_ok=True)\n",
        "os.makedirs('plots', exist_ok=True)\n",
        "print(\"✅ Created directories: saved_models/, plots/\")\n",
        "\n",
        "\n",
        "cats_dir = os.path.join(DATA_DIR, 'cats')\n",
        "dogs_dir = os.path.join(DATA_DIR, 'dogs')\n",
        "\n",
        "if not os.path.exists(cats_dir) or not os.path.exists(dogs_dir):\n",
        "    print(\"❌ Data directories not found!\")\n",
        "    print(\"Please run the following in a terminal:\")\n",
        "    print(\"   python download_data.py\")\n",
        "    print(\"Or manually create:\")\n",
        "    print(f\"   {cats_dir}/\")\n",
        "    print(f\"   {dogs_dir}/\")\n",
        "    raise FileNotFoundError(\"Data not found\")\n",
        "\n",
        "print(\"✅ Data directories found!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "📁 Loading dataset...\n",
            "Loading cat images...\n",
            "Loading dog images...\n",
            "Loaded 160 training images and 40 validation images\n",
            "Training: 77 cats, 83 dogs\n",
            "Validation: 23 cats, 17 dogs\n",
            "\n",
            "📊 Dataset Summary:\n",
            "   Training images: 160\n",
            "   Validation images: 40\n",
            "   Image shape: (3, 28, 28)\n",
            "   Training batches: 5\n",
            "   Validation batches: 2\n"
          ]
        }
      ],
      "source": [
        "\n",
        "print(\"📁 Loading dataset...\")\n",
        "\n",
        "data_loader = DataLoader(\n",
        "    cats_dir=cats_dir,\n",
        "    dogs_dir=dogs_dir,\n",
        "    batch_size=BATCH_SIZE,\n",
        "    target_size=(IMAGE_SIZE, IMAGE_SIZE),\n",
        "    validation_split=0.2,\n",
        "    max_images_per_class=MAX_IMAGES_PER_CLASS\n",
        ")\n",
        "\n",
        "\n",
        "dataset_info = data_loader.get_dataset_info()\n",
        "\n",
        "print(\"\\n📊 Dataset Summary:\")\n",
        "print(f\"   Training images: {dataset_info['train_size']}\")\n",
        "print(f\"   Validation images: {dataset_info['val_size']}\")\n",
        "print(f\"   Image shape: {dataset_info['input_shape']}\")\n",
        "print(f\"   Training batches: {dataset_info['train_batches']}\")\n",
        "print(f\"   Validation batches: {dataset_info['val_batches']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🧠 Creating CNN model...\n",
            "✅ Model created with 119,809 parameters\n",
            "\n",
            "🎯 OPTIMIZED Architecture (80% accuracy target):\n",
            "   1. Conv2D(32) → ReLU → MaxPool2D\n",
            "   2. Conv2D(64) → ReLU → MaxPool2D\n",
            "   3. Flatten\n",
            "   4. Dense(32) → ReLU → Dropout(0.3)\n",
            "   5. Dense(1) → Sigmoid\n",
            "⚡ Designed for 80%+ accuracy with reasonable speed!\n",
            "\n",
            "⚙️ Using Adam optimizer (lr=0.003)\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# 🧠 Create the CNN Model\n",
        "# ===================================================================\n",
        "\n",
        "print(\"🧠 Creating CNN model...\")\n",
        "\n",
        "\n",
        "model = CNN(input_shape=dataset_info['input_shape'], num_classes=1)\n",
        "\n",
        "print(f\"✅ Model created with {model.get_num_parameters():,} parameters\")\n",
        "print(\"\\n🎯 OPTIMIZED Architecture (80% accuracy target):\")\n",
        "print(\"   1. Conv2D(32) → ReLU → MaxPool2D\")\n",
        "print(\"   2. Conv2D(64) → ReLU → MaxPool2D\") \n",
        "print(\"   3. Flatten\")\n",
        "print(\"   4. Dense(32) → ReLU → Dropout(0.3)\")\n",
        "print(\"   5. Dense(1) → Sigmoid\")\n",
        "print(\"⚡ Designed for 80%+ accuracy with reasonable speed!\")\n",
        "\n",
        "\n",
        "optimizer = Adam(learning_rate=LEARNING_RATE)\n",
        "print(f\"\\n⚙️ Using Adam optimizer (lr={LEARNING_RATE})\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✅ Training functions defined!\n"
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# 🏋️ Training Functions\n",
        "# ===================================================================\n",
        "\n",
        "def train_epoch(model, data_loader, optimizer):\n",
        "    \"\"\"Train for one epoch\"\"\"\n",
        "    model.train_mode()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_images, batch_labels in data_loader.get_batch('train'):\n",
        "        # Forward pass\n",
        "        predictions = model.forward(batch_images)\n",
        "        loss, dout = model.compute_loss(predictions, batch_labels)\n",
        "        \n",
        "        # Backward pass\n",
        "        model.backward(dout)\n",
        "        model.update_parameters(optimizer)\n",
        "        \n",
        "        # Calculate accuracy\n",
        "        batch_preds = (predictions > 0.5).astype(int)\n",
        "        batch_acc = calculate_accuracy(batch_preds, batch_labels)\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += batch_acc\n",
        "        num_batches += 1\n",
        "    \n",
        "    return epoch_loss / num_batches, epoch_acc / num_batches\n",
        "\n",
        "def validate_epoch(model, data_loader):\n",
        "    \"\"\"Validate for one epoch\"\"\"\n",
        "    model.eval_mode()\n",
        "    \n",
        "    epoch_loss = 0.0\n",
        "    epoch_acc = 0.0\n",
        "    num_batches = 0\n",
        "    \n",
        "    for batch_images, batch_labels in data_loader.get_batch('val'):\n",
        "        predictions = model.forward(batch_images)\n",
        "        loss, _ = model.compute_loss(predictions, batch_labels)\n",
        "        \n",
        "        batch_preds = (predictions > 0.5).astype(int)\n",
        "        batch_acc = calculate_accuracy(batch_preds, batch_labels)\n",
        "        \n",
        "        epoch_loss += loss\n",
        "        epoch_acc += batch_acc\n",
        "        num_batches += 1\n",
        "    \n",
        "    model.train_mode()\n",
        "    return epoch_loss / num_batches, epoch_acc / num_batches\n",
        "\n",
        "print(\"✅ Training functions defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "🚀 Starting training...\n",
            "============================================================\n",
            "\n",
            "📈 Epoch 1/8\n",
            "----------------------------------------\n",
            "   Train: Loss=0.7538, Acc=0.5188\n",
            "   Val:   Loss=0.7015, Acc=0.5469\n",
            "   Time: 78.4s\n",
            "Model saved to saved_models/best_model.pkl\n",
            "   🎯 New best accuracy: 0.5469 - Model saved!\n",
            "\n",
            "📈 Epoch 2/8\n",
            "----------------------------------------\n",
            "   Train: Loss=0.6945, Acc=0.5000\n",
            "   Val:   Loss=0.7000, Acc=0.4531\n",
            "   Time: 78.1s\n",
            "\n",
            "📈 Epoch 3/8\n",
            "----------------------------------------\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[7], line 25\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m📈 Epoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mEPOCHS\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m40\u001b[39m)\n\u001b[0;32m---> 25\u001b[0m train_loss, train_acc \u001b[38;5;241m=\u001b[39m train_epoch(model, data_loader, optimizer)\n\u001b[1;32m     26\u001b[0m val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate_epoch(model, data_loader)\n\u001b[1;32m     29\u001b[0m train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
            "Cell \u001b[0;32mIn[6], line 19\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[0;34m(model, data_loader, optimizer)\u001b[0m\n\u001b[1;32m     16\u001b[0m loss, dout \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mcompute_loss(predictions, batch_labels)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m model\u001b[38;5;241m.\u001b[39mbackward(dout)\n\u001b[1;32m     20\u001b[0m model\u001b[38;5;241m.\u001b[39mupdate_parameters(optimizer)\n\u001b[1;32m     22\u001b[0m \u001b[38;5;66;03m# Calculate accuracy\u001b[39;00m\n",
            "File \u001b[0;32m~/Downloads/AI_Projects/elham-stuff/src/cnn/network.py:83\u001b[0m, in \u001b[0;36mCNN.backward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     81\u001b[0m gradient \u001b[38;5;241m=\u001b[39m dout\n\u001b[1;32m     82\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mreversed\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayers):\n\u001b[0;32m---> 83\u001b[0m     gradient \u001b[38;5;241m=\u001b[39m layer\u001b[38;5;241m.\u001b[39mbackward(gradient)\n",
            "File \u001b[0;32m~/Downloads/AI_Projects/elham-stuff/src/cnn/layers.py:102\u001b[0m, in \u001b[0;36mConvLayer.backward\u001b[0;34m(self, dout)\u001b[0m\n\u001b[1;32m     99\u001b[0m                 dw[f, :, :, :] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m x_padded[n, :, h_start:h_end, w_start:w_end] \u001b[38;5;241m*\u001b[39m dout[n, f, i, j]\n\u001b[1;32m    101\u001b[0m                 \u001b[38;5;66;03m# Gradient w.r.t input\u001b[39;00m\n\u001b[0;32m--> 102\u001b[0m                 dx_padded[n, :, h_start:h_end, w_start:w_end] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights[f, :, :, :] \u001b[38;5;241m*\u001b[39m dout[n, f, i, j]\n\u001b[1;32m    104\u001b[0m \u001b[38;5;66;03m# Gradient w.r.t bias\u001b[39;00m\n\u001b[1;32m    105\u001b[0m db \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msum(dout, axis\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m3\u001b[39m))\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_filters, \u001b[38;5;241m1\u001b[39m)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# ===================================================================\n",
        "# 🚀 Start Training!\n",
        "# ===================================================================\n",
        "\n",
        "print(\"🚀 Starting training...\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "\n",
        "train_losses = []\n",
        "val_losses = []\n",
        "train_accs = []\n",
        "val_accs = []\n",
        "best_val_acc = 0.0\n",
        "\n",
        "\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    epoch_start = time.time()\n",
        "    \n",
        "    print(f\"\\n📈 Epoch {epoch + 1}/{EPOCHS}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "\n",
        "    train_loss, train_acc = train_epoch(model, data_loader, optimizer)\n",
        "    val_loss, val_acc = validate_epoch(model, data_loader)\n",
        "    \n",
        "\n",
        "    train_losses.append(train_loss)\n",
        "    val_losses.append(val_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    val_accs.append(val_acc)\n",
        "    \n",
        "\n",
        "    epoch_time = time.time() - epoch_start\n",
        "    \n",
        "\n",
        "    print(f\"   Train: Loss={train_loss:.4f}, Acc={train_acc:.4f}\")\n",
        "    print(f\"   Val:   Loss={val_loss:.4f}, Acc={val_acc:.4f}\")\n",
        "    print(f\"   Time: {epoch_time:.1f}s\")\n",
        "    \n",
        "\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        model.save_model('saved_models/best_model.pkl')\n",
        "        print(f\"   🎯 New best accuracy: {best_val_acc:.4f} - Model saved!\")\n",
        "    \n",
        "\n",
        "    if PLOT_DURING_TRAINING and (epoch + 1) % 3 == 0:\n",
        "        clear_output(wait=True)\n",
        "        \n",
        "\n",
        "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))\n",
        "        \n",
        "        ax1.plot(train_losses, 'b-', label='Train Loss')\n",
        "        ax1.plot(val_losses, 'r-', label='Val Loss')\n",
        "        ax1.set_title('Training Loss')\n",
        "        ax1.set_xlabel('Epoch')\n",
        "        ax1.set_ylabel('Loss')\n",
        "        ax1.legend()\n",
        "        ax1.grid(True)\n",
        "        \n",
        "        ax2.plot(train_accs, 'b-', label='Train Acc')\n",
        "        ax2.plot(val_accs, 'r-', label='Val Acc')\n",
        "        ax2.set_title('Training Accuracy')\n",
        "        ax2.set_xlabel('Epoch')\n",
        "        ax2.set_ylabel('Accuracy')\n",
        "        ax2.legend()\n",
        "        ax2.grid(True)\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        print(f\"📈 Epoch {epoch + 1}/{EPOCHS} completed\")\n",
        "        print(f\"   Current: Train={train_acc:.3f}, Val={val_acc:.3f}\")\n",
        "        print(f\"   Best Val Acc: {best_val_acc:.3f}\")\n",
        "\n",
        "\n",
        "total_time = time.time() - start_time\n",
        "print(f\"\\n🎉 Training completed in {total_time:.1f} seconds!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 📊 Final Results and Plots\n",
        "# ===================================================================\n",
        "\n",
        "\n",
        "model.save_model('saved_models/final_model.pkl')\n",
        "\n",
        "\n",
        "if SAVE_PLOTS:\n",
        "    plot_path = 'plots/training_history.png'\n",
        "    plot_training_history(train_losses, val_losses, train_accs, val_accs, plot_path)\n",
        "\n",
        "\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
        "\n",
        "ax1.plot(train_losses, 'b-', label='Training Loss', linewidth=2)\n",
        "ax1.plot(val_losses, 'r-', label='Validation Loss', linewidth=2)\n",
        "ax1.set_title('Training and Validation Loss', fontsize=14)\n",
        "ax1.set_xlabel('Epoch')\n",
        "ax1.set_ylabel('Loss')\n",
        "ax1.legend()\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(train_accs, 'b-', label='Training Accuracy', linewidth=2)\n",
        "ax2.plot(val_accs, 'r-', label='Validation Accuracy', linewidth=2)\n",
        "ax2.set_title('Training and Validation Accuracy', fontsize=14)\n",
        "ax2.set_xlabel('Epoch')\n",
        "ax2.set_ylabel('Accuracy')\n",
        "ax2.legend()\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "print(\"=\" * 60)\n",
        "print(\"🎉 TRAINING SUMMARY\")\n",
        "print(\"=\" * 60)\n",
        "print(f\"📈 Final Results:\")\n",
        "print(f\"   Final train accuracy: {train_accs[-1]:.4f}\")\n",
        "print(f\"   Final validation accuracy: {val_accs[-1]:.4f}\")\n",
        "print(f\"   Best validation accuracy: {best_val_acc:.4f}\")\n",
        "print(f\"\\n💾 Files saved:\")\n",
        "print(f\"   Best model: saved_models/best_model.pkl\")\n",
        "print(f\"   Final model: saved_models/final_model.pkl\")\n",
        "if SAVE_PLOTS:\n",
        "    print(f\"   Training plot: plots/training_history.png\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 🧪 Quick Test on Validation Data\n",
        "# ===================================================================\n",
        "\n",
        "print(\"🧪 Testing model on validation data...\")\n",
        "\n",
        "# Get validation data\n",
        "val_images, val_labels = data_loader.get_full_dataset('val')\n",
        "\n",
        "# Test on first 20 samples\n",
        "test_images = val_images[:20]\n",
        "test_labels = val_labels[:20]\n",
        "\n",
        "# Make predictions\n",
        "predictions, probabilities = model.predict(test_images)\n",
        "\n",
        "# Calculate accuracy\n",
        "correct = np.sum(predictions.flatten() == test_labels.flatten())\n",
        "accuracy = correct / len(test_labels)\n",
        "\n",
        "print(f\"✅ Accuracy on 20 validation samples: {correct}/20 = {accuracy:.2f}\")\n",
        "\n",
        "# Show some predictions\n",
        "class_names = ['Cat', 'Dog']\n",
        "print(f\"\\n🔍 Sample predictions:\")\n",
        "for i in range(min(5, len(predictions))):\n",
        "    true_label = int(test_labels[i])\n",
        "    pred_label = int(predictions[i])\n",
        "    confidence = float(probabilities[i])\n",
        "    \n",
        "    if pred_label == 0:\n",
        "        confidence = 1 - confidence\n",
        "    \n",
        "    status = \"✅\" if true_label == pred_label else \"❌\"\n",
        "    print(f\"   {status} True: {class_names[true_label]}, Pred: {class_names[pred_label]} ({confidence:.3f})\")\n",
        "\n",
        "print(f\"\\n🎯 Your model is ready for use!\")\n",
        "print(f\"   Run the next cells to make predictions on new images!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ===================================================================\n",
        "# 🔮 Make Predictions on Sample Images\n",
        "# ===================================================================\n",
        "\n",
        "from data import ImagePreprocessor\n",
        "\n",
        "# Create preprocessor\n",
        "preprocessor = ImagePreprocessor(target_size=(IMAGE_SIZE, IMAGE_SIZE))\n",
        "\n",
        "# Test on some images from the validation set\n",
        "print(\"🔮 Making predictions on sample images...\")\n",
        "\n",
        "# Get a few random validation images\n",
        "val_images, val_labels = data_loader.get_full_dataset('val')\n",
        "sample_indices = np.random.choice(len(val_images), size=min(8, len(val_images)), replace=False)\n",
        "sample_images = val_images[sample_indices]\n",
        "sample_labels = val_labels[sample_indices]\n",
        "\n",
        "# Make predictions\n",
        "predictions, probabilities = model.predict(sample_images)\n",
        "\n",
        "# Display results\n",
        "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
        "axes = axes.flatten()\n",
        "\n",
        "class_names = ['Cat', 'Dog']\n",
        "\n",
        "for i in range(len(sample_images)):\n",
        "    # Convert image for display\n",
        "    img = np.transpose(sample_images[i], (1, 2, 0))\n",
        "    if img.max() <= 1.0:\n",
        "        img = (img * 255).astype(np.uint8)\n",
        "    \n",
        "    axes[i].imshow(img)\n",
        "    \n",
        "    # Get prediction info\n",
        "    true_label = int(sample_labels[i])\n",
        "    pred_label = int(predictions[i])\n",
        "    confidence = float(probabilities[i])\n",
        "    \n",
        "    if pred_label == 0:\n",
        "        confidence = 1 - confidence\n",
        "    \n",
        "    # Create title\n",
        "    title = f'True: {class_names[true_label]}\\nPred: {class_names[pred_label]} ({confidence:.2f})'\n",
        "    color = 'green' if true_label == pred_label else 'red'\n",
        "    axes[i].set_title(title, color=color, fontsize=10)\n",
        "    axes[i].axis('off')\n",
        "\n",
        "# Hide empty subplots\n",
        "for i in range(len(sample_images), 8):\n",
        "    axes[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"✅ Predictions completed!\")\n",
        "print(\"🎉 Your CNN classifier is working!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
